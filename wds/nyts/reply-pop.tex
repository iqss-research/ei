\documentclass[11pt]{article}
\usepackage{graphicx}
\addtolength{\oddsidemargin}{-.55in}
\addtolength{\evensidemargin}{-.55in}
\addtolength{\textwidth}{1.1in}
\addtolength{\textheight}{.9in}

\begin{document}

\begin{center}
  {\bf \Large Point-By-Point Reply to Referee~1}
\end{center}

We begin by thanking the two referee for his/her careful reading of
our manuscript and their many helpful comments. We believe we have
addressed all of these comments in our revised manuscript and that in
so doing we have improved our presentation significantly.  In what
follows, the original comments appear in italic type; our replies and
comments are in roman type.

\bigskip

\noindent {\bf Major Comments of and reply to Referee~1}

\begin{enumerate}
  
\item {\it Using 67 observations, the author wants to estimate the
    impact of 27 covariates and $X_i$ on bad vote rates for Bush and
    Gore.  Each covariate requires 2 estimated parameters as in the
    ``Except [for] the first three models $\cdots$'' clause of
    footnote 9.  Roughly, this means that there are 56 parameters to
    estimate.  $67-56=11$, and this is a very, very low number. This
    paper, therefore, has almost no data to bear on the quantities it
    wants to estimate.  This is not the fault of the authors.  The bad
    votes problem is just one that is practically data-less.}
  
  \smallskip The referee seems to misunderstand the primary goal of
  our paper.  The purpose of our method is NOT to ``estimate the
  impact of 27 covariates and $X_i$ on bad vote rates for Bush and
  Gore.'' Rather, our goal is to predict the number of bad votes that
  was cast for Bush. These two are fundamentally different: the first
  is the problem of causal inference, while the second is the problem
  of the predictive inference. In
  addition, the fundamental theoretical property of Bayesian model
  averaging is concerned about the predictive inference and not about
  the causal inference. We will elaborate more on this point below.
  
  In predictive problems (but not causal problems) where we do not
  know the true model, estimating a model with ``all'' variables one
  can think of will almost surely generate bias and inefficiency
  relative to some subset.  In particular, including irrelevant
  variables may not bias causal inferences but they will bias
  predictive inferences and so should be avoided.  This is the
  conclusion of the large literature on model (or variable) selection
  (e.g., stepwise methods, Cp criterion, etc.), most of which don't
  make much sense in making causal inferences. 
  
  A key result in this literature is that BMA is known to perform
  better than all these standard variable selection procedures because
  it takes into account model uncertainty rather than trying to select
  the ``one best model.''  Hoeting et al (1999, Statistical Science)
  and Madigan and Raftery (1994, JASA) among others have shown that
  BMA almost never puts much weight on the model with all covariates,
  even when that is an option, and that the average of the submodels
  is optimal.  Indeed, even if the true model has 2000 variables --
  and so with 67 observations it can't be estimated, and for
  predictive purposes some model with a subset of variables must be
  chosen -- a BMA that averages over any relevant set of submodels
  will outperform any individual model in the set, even though the
  truth is obviously not one of the models estimated.  
 
\item {\it If I were reviewing a paper that used a least squares model
    with 67 observations to estimate 56 parameters, I would not trust
    its answers at all.  In this case, the situation is even harder.
    The models used in the paper are non-linear and use aggregate
    data.
    
    The author tries to get around this problem by using model
    averaging.  Rather than one EI model with 56 estimated parameters
    (or more, for intercepts and variances) the author considers a
    collection of EI models that require only a few parameters.  This
    does not eliminate the problem of estimating 56 parameters with 67
    observations.  Furthermore, the author does averaging over a tiny
    subset of possible models.  There are roughly 228 (or maybe 256)
    models over which the author should be averaging.  However, he
    considers only 31 of these.  31/228 is basically zero, and so it
    follows that the author has not explored the model space. 
    
    Will the author's approach, which calls for using 31 small models,
    be sufficient to cover the entire model space?  I cannot imagine
    so.  There are no arguments for this in the paper.  The author
    claims on page 10 that the reduction to 31 is fine, but there is
    no evidence showing this to be the case.}
  
  As the referee correctly points out, we have not explored the entire
  model space. Indeed, in our application, neither 31 nor 228 models
  do not represent the entire model space. One can collect more
  covariates, include more interactions and higher order polynomial
  terms, use different ecological inference models, and so on.  Since
  the model space is infinite, however, one can never explore the
  entire model space.  Bayesian model averaging (BMA), therefore, must
  condition on a subset of the entire model space. In the revised
  manuscript, we emphasized this weakness.
  
  However, we disagree with the referee's point that this invalidates
  our BMA analysis. If the referee's criticism is correct, any
  standard statistical analysis is invalid since it conditions on one
  model.  For example, the standard ecological inference analysis
  would use one model with no covariate.  The referee's suggestion to
  ``consider EI models that include $X_i$ only'' is also suboptimal
  since it conditions on a single model and does not explore the
  entire model space. 
  
  Our BMA analysis does not explore the entire model space, but is
  superior to any of these standard approach. This is based on the
  general theoretical property of BMA. The BMA analysis is better at
  estimating a predictive quantity than any submodel being averaged
  over, even if the true model is {\it outside} the set of submodels
  being averaged over (e.g., Madigan and Raftery, 1994 JASA).  Almost
  the same result exists in the literature on committee methods (e.g.,
  Bishop, 1995). This implies that our BMA approach is better than any
  submodel that is averaged over in our analysis (see Appendix B.2).
  
  Of course, someone could find a better submodel to include in a BMA.
  No one has suggested a plausible story, and accompanying model, but
  we are certainly open to be proved wrong if someone finds such a
  model.  However, this fact does not distinguish our analysis from
  any other statistical analysis in any field.  Every empirical
  analysis is vulnerable to this criticism that there might be a
  better model that is not considered in the original analysis.
  
\item {\it Notwithstanding the data-less aspect of the overseas
    problem, much more should be said about the EI method being used.
    King's EI technique has been the subject of at least four
    controversies: the Tam article in PA, the JASA exchange, the
    American Statistician exchange, and the PA exchange.  I think the
    most relevant of these are the first two.  Any application of
    King's EI method needs to note these exchanges, as they talk about
    the importance of assumptions and limitations of the method.  It
    is fine for the paper to depend on King's EI method since it must
    depend on something.  But it needs to be honest with known model
    shortcomings.}
  
\item {\it It also needs to be honest about priors.  When a Bayesian
    statistical method has 67 observations, priors are going to matter
    a huge amount, even if only 5 parameters were estimated.  The
    author on page 12 says that priors should ``include all
    information available about a quantity of interest'' but then goes
    on to use ``standard independent priors.'' These standard priors
    have nothing to do with information.  Any prior that is standard
    cannot reflect information unique to a given problem, else it
    would not be standard.}
  
  There are two kinds of prior that we specify in our BMA analysis.
  One on the model probability and the other on the distribution of
  parameters. We use the noninformative prior for the model
  probability by not favoring any particular model a priori. This is a
  standard approach in BMA since the whole purpose of BMA is to let
  the data tell you which model should be preferred. For the model
  parameters, we use the standard prior presented and examined in King
  (1997). Following the standard Bayesian analysis, we conduct the
  sensitivity analysis (see Appendix B.2) and find that prior
  specification had little influence on the outcome.
  
\item {\it For a revision of this paper, I would recommend sticking
    with the bounds analysis which is excellent and nicely written.}
  
  The bounds analysis is appealing since it does not impose any
  statistical assumptions.  However, the public is apparently
  uninterested in the limited information it provides, and so the
  bounds alone do not answer the question posed (this is of course why
  the field of ecological inference did not end in 1953 with Duncan
  and Davis' discovery of the bounds, and why the bounds alone are
  used in very few studies studies that apply ecological inference
  even in academia).  
  
  Indeed, the Times reporters constantly told us that they needed a
  point estimate, and that they could deal with confidence intervals
  (or margins of error) but not the bounds.  BMA provides a point
  estimate that takes into account all reasonable models that were
  suggested to us at that time.  It has limitations, but it is better
  than any feasible alternative suggested at the time or since in
  providing what the public and newspaper needed (a point estimate and
  uncertainty estimate). 
 
\item {\it I would also keep the material on Republican pressure,
    although the conclusions the author draws from this are overstated
    in some places.  The Republicans choose where to pressure, so
    arguing that they were successful (see page 19) is hard to
    justify.}
  
  In the revised manuscript, we have acknowledged this possible
  selection bias and added a qualification. See page ??.


\end{enumerate}

\bigskip
\noindent {\bf Minor Comments of and reply to Referee~1}

\begin{enumerate}
\item {\it The first sentence of the paper is awkward.  ``Yet, they
    clearly determined the outcome of the election.'' This can be said
    of every anomaly from Florida given the tight margin.}
  
\item {\it ``Our results give ...'' on page 3.  Replace ``give'' with
    ``estimate.'' Statistical results can do not better than this.}
  
\item {\it ``seen as remarkable ...'' is not remarkable.  The
    probability that all voters in Florida like Gore more than Bush
    has positive probability, the probability that Katherine Harris
    decided the election has positive probability, and so on.}
  
\item {\it ``In counties more favorable to Democrats ...'' examples
    would be nice.}
  
\item {\it Table 2 has 3 columns and Table 3 has 2 columns.  The
    transition is awkward.}
  
\item {\it Page 7 should cite Duncan and Davis.}
  
\item {\it The Baker County example on page 8 is striking and could be
    played up more.}
  
\item {\it ``... knowing for certain who won the 2000 election'' We
    all know that Bush won since winning is a legal matter.  What the
    author means is knowing if more voters were cast for Bush than
    Gore in Florida.}
  
\item {\it Section 4.3 has too many comments that look causal.  See my
    comments above.  ``if all American laws had been followed'' is not
    the point here.  Voting laws are almost entirely local, so the
    issue for Florida votes is Florida laws.}


\end{enumerate}

\clearpage
\begin{center}
  {\bf \Large Point-By-Point Reply to Referee~2}
\end{center}

We begin by thanking the referee for his/her careful reading of our
manuscript and many helpful comments. We believe we have addressed all
of these comments in our revised manuscript and that in so doing we
have improved our presentation significantly.  In what follows, the
original comments appear in italic type; our replies and comments are
in roman type.

\bigskip

\noindent {\bf Major Comments of and reply to Referee~2}

\begin{enumerate}
\item {\it This paper, like some others I have seen, seems to assume
    that any question touching on the 2000 presidential vote in
    Florida is inherently fascinating.  But the question here is
    whether voiding 680 illegal ballots would have reversed a 537-vote
    margin.  For that to be the case, the illegal ballots would have
    had to split about 90-10 for Bush.  How likely is a 90-10 split in
    a 50-50 state?  For that matter, how likely is a 90-10 split in
    presidential votes in ANY social group?  Unless they happened to
    be a boatload of Christian fundamentalist missionaries, not very.
    We don't need ecological inference, Bayesian model averaging, or
    high-priced political science of any kind to conclude that the
    authors' question is not really all that interesting.}
  
  We do not want to discuss whether the question posed by the NYT is
  interesting since it is purely a matter of subjective judgment.
  Rather, our goal was to give a scientific answer to this question.
  Having said that, a few points need to be clarified.  First, note
  that $537/680 \approx 0.78$ and hence it is 80-20 split rather than
  90-10 split. Second, we are looking at the Bush's vote share among
  {\it bad} overseas ballots rather than all overseas ballots. More
  than 25 NYT journalists spent six months gathering massive
  qualitative evidence that the Bush campaign put significant amount
  of pressure on local electoral officials to count those ballots that
  were likely to be cast for Bush (e.g., military votes).  Based on
  this evidence, they concluded that it may be possible that 80\% of
  illegally counted overseas ballots is cast for Bush, and asked us to
  investigate such possibility. The NYT then wrote the full four page
  article - the longest since the Watergate - on this question and our
  analysis. We believe that the NYT question is reasonable. In
  Escambia, one of Bush's strongholds, about 78\% of all overseas
  ballots were cast for Bush. If the Bush campaign's strategy is very
  successful, 80-20 split is certainly possible if not highly likely.

  
\item {\it The authors evade this problem partly through misdirection
    and partly through purple prose.  For example, on page 3, the
    estimated probability that Gore should have won "is small, but we
    know with mathematical certainty that it is greater than zero--a
    conclusion that can only be seen as remarkable for the world's
    premier democracy."  What does this mean?
    
    Since the number of invalid ballots was (according to the New York
    Times) larger than the vote margin, and since we don't know how
    the invalid ballots were cast, it is impossible in principle for
    ANY statistical analysis to assign a probability of zero (or one)
    to the proposition that Gore should have won.  I suppose that is a
    ``mathematical certainty,'' but it is not a very interesting one,
    and it owes nothing to the authors' analysis.  Nor is it very
    remarkable.  Any coherent (i.e., Bayesian) worldview would, I
    think, have to assign non-zero (albeit VERY small) probability to
    the proposition that the candidate won EVERY presidential election
    in American history due to erroneous or fraudulent vote counts.
    The possibility doesn't keep me awake at night.}
  
  By ``mathematical certainty,'' we mean that the data alone (without
  any statistical assumption) cannot tell whether Gore should have won
  the election. This is not because ``the number of invalid ballots
  was larger than the vote margin.'' Indeed, the analysis of bounds
  can eliminate the possibility of Gore's victory. Since the analysis
  of bounds does not impose any statistical assumption, we believe
  that it is important to stress its conclusion. In fact, this kind of
  identification analysis is a common practice in modern applied
  statistics (see e.g., the works by Charles Manski).
  
\item {\it When I first saw the paper I was naturally intrigued by the
    claim that "Formal Bayesian model averaging has not been used in
    political science."  I thought that scholars had used it over the
    past half-dozen years.  It turns out (footnote 7) that they used
    "an approximation to formal Bayesian model averaging."  Why an
    "approximation"?  The authors never say, but I conjecture from
    some discussion in Appendix A that what they have in mind is that
    previous work employed the BIC statistic to index the relative
    likelihood of alternative models, whereas they use a Laplace
    approximation.  (Isn't an "approximation" too?)  The Laplace
    approximation is, indeed, more appropriate for their application,
    but identical for others' (i.e., for ordinary regression models).
    So the primary methodological selling point of the paper seems to
    me to reduce, essentially, to tendentious self-promotion.}

  We meant that the Laplace approximation provides a better way to
  approximate the marginal likelihood. This is the conclusion of the
  recent statistical literature (see e.g., ). To avoid the
  misunderstanding, we changed this sentence to ...
  
\item {\it What seems most striking about the statistical analysis (pages
  9-14) is the high ratio of wind-up to substance.  The authors
  provide nice general descriptions of Gary King's EI model and of
  Bayesian model averaging, but make NO mention in the text of what
  models they actually estimated (these are listed very
  telegraphically in a footnote) or what the results were (even in the
  appendices)!  This must come close to absolute zero with respect to
  a methodological standard that Gary King has admirably set forth in
  his work: make it clear what the data are and how you got from the
  data to the conclusions.}

\item {\it The model space requires much more in the way of
    substantive justification than it gets here.  I see no need to
    consider every logically possible combination of the 27 available
    covariates, but neither does it make sense to limit the analysis
    to bivariate models.  Saying more about what the variables
    actually measure, why they might be relevant, and which (if any)
    produce significant results when considered in isolation and in
    limited, substantively sensible combinations would be a good start
    on a more interesting and potentially convincing analysis.
    Whether this problem deserves that sort of work is unclear to me,
    but that sort of work would, in my view, be necessary for the
    statistical analysis to fly.}
 


 


\end{enumerate}

\end{document}


